{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SNAIL Pytorch Quick Draw",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jt658/CS330-Final-Project/blob/main/SNAIL_Pytorch_Quick_Draw.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RA_ZXjREUa1W"
      },
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile(\"/dataset/QuickDrawData.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall('QuickDrawData')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZCIaX5AAtgq"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "class BatchSampler(object):\n",
        "    '''\n",
        "    BatchSampler: yield a batch of indexes at each iteration.\n",
        "    __len__ returns the number of episodes per epoch (same as 'self.iterations').\n",
        "    '''\n",
        "\n",
        "    def __init__(self, labels, classes_per_it, num_samples, iterations, batch_size):\n",
        "        '''\n",
        "        Initialize the BatchSampler object\n",
        "        Args:\n",
        "        - labels: an iterable containing all the labels for the current dataset\n",
        "        samples indexes will be infered from this iterable.\n",
        "        - classes_per_it: number of random classes for each iteration\n",
        "        - num_samples: number of samples for each iteration for each class\n",
        "        - iterations: number of iterations (episodes) per epoch\n",
        "        '''\n",
        "        super(BatchSampler, self).__init__()\n",
        "        self.labels = labels\n",
        "        self.classes_per_it = classes_per_it\n",
        "        self.sample_per_class = num_samples\n",
        "        self.iterations = iterations\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.classes, self.counts = np.unique(self.labels, return_counts=True)\n",
        "\n",
        "        self.idxs = range(len(self.labels))\n",
        "        self.label_tens = np.empty((len(self.classes), max(self.counts)), dtype=int) * np.nan\n",
        "        self.label_lens = np.zeros_like(self.classes)\n",
        "        for idx, label in enumerate(self.labels):\n",
        "            label_idx = np.argwhere(self.classes == label)[0, 0]\n",
        "            self.label_tens[label_idx, np.where(np.isnan(self.label_tens[label_idx]))[0][0]] = idx\n",
        "            self.label_lens[label_idx] += 1\n",
        "\n",
        "    def __iter__(self):\n",
        "        '''\n",
        "        yield a batch of indexes\n",
        "        '''\n",
        "        spc = self.sample_per_class + 1 # To get that extra sample\n",
        "        cpi = self.classes_per_it\n",
        "        num_samples = spc * cpi\n",
        "        true_num_samples = (spc - 1) * cpi + 1\n",
        "\n",
        "        for it in range(self.iterations):\n",
        "            total_batch = np.array([])\n",
        "            for _ in range(self.batch_size):\n",
        "                batch = np.empty(num_samples)\n",
        "                c_idxs = np.random.permutation(len(self.classes))[:cpi]\n",
        "                for i, c in enumerate(self.classes[c_idxs]):\n",
        "                    s = slice(i, i + num_samples, cpi)\n",
        "                    label_idx = np.argwhere(self.classes == c)[0, 0]\n",
        "                    if spc > self.label_lens[label_idx]:\n",
        "                        raise AssertionError('More samples per class than exist in the dataset')\n",
        "                    sample_idxs = np.random.permutation(self.label_lens[label_idx])[:spc]\n",
        "                    batch[s] = self.label_tens[label_idx][sample_idxs]\n",
        "                offset = random.randint(0, 4)\n",
        "                batch = batch[offset:offset + true_num_samples]\n",
        "                batch[:true_num_samples - 1] = batch[:true_num_samples - 1][np.random.permutation(true_num_samples - 1)]\n",
        "                total_batch = np.append(total_batch, batch)\n",
        "            yield total_batch.astype(int)\n",
        "\n",
        "    def __len__(self):\n",
        "        '''\n",
        "        returns the number of iterations (episodes) per epoch\n",
        "        '''\n",
        "        return self.iterations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01Hdi0ZjA3S7"
      },
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CasualConv1d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, \n",
        "                 stride=1, dilation=1, groups=1, bias=True):\n",
        "        super(CasualConv1d, self).__init__()\n",
        "        self.dilation = dilation\n",
        "        padding = dilation * (kernel_size - 1)\n",
        "        self.conv1d = nn.Conv1d(in_channels, out_channels, kernel_size, stride,\n",
        "                                padding, dilation, groups, bias)\n",
        "\n",
        "    def forward(self, input):\n",
        "        # Takes something of shape (N, in_channels, T),\n",
        "        # returns (N, out_channels, T)\n",
        "        out = self.conv1d(input)\n",
        "        return out[:, :, :-self.dilation] # TODO: make this correct for different strides/padding\n",
        "\n",
        "class DenseBlock(nn.Module):\n",
        "    def __init__(self, in_channels, dilation, filters, kernel_size=2):\n",
        "        super(DenseBlock, self).__init__()\n",
        "        self.casualconv1 = CasualConv1d(in_channels, filters, kernel_size, dilation=dilation)\n",
        "        self.casualconv2 = CasualConv1d(in_channels, filters, kernel_size, dilation=dilation)\n",
        "\n",
        "    def forward(self, input):\n",
        "        # input is dimensions (N, in_channels, T)\n",
        "        xf = self.casualconv1(input)\n",
        "        xg = self.casualconv2(input)\n",
        "        activations = F.tanh(xf) * F.sigmoid(xg) # shape: (N, filters, T)\n",
        "        return torch.cat((input, activations), dim=1)\n",
        "        \n",
        "class TCBlock(nn.Module):\n",
        "    def __init__(self, in_channels, seq_length, filters):\n",
        "        super(TCBlock, self).__init__()\n",
        "        self.dense_blocks = nn.ModuleList([DenseBlock(in_channels + i * filters, 2 ** (i+1), filters)\n",
        "                                           for i in range(int(math.ceil(math.log(seq_length, 2))))])\n",
        "\n",
        "    def forward(self, input):\n",
        "        # input is dimensions (N, T, in_channels)\n",
        "        input = torch.transpose(input, 1, 2)\n",
        "        for block in self.dense_blocks:\n",
        "            input = block(input)\n",
        "        return torch.transpose(input, 1, 2)\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, in_channels, key_size, value_size):\n",
        "        super(AttentionBlock, self).__init__()\n",
        "        self.linear_query = nn.Linear(in_channels, key_size)\n",
        "        self.linear_keys = nn.Linear(in_channels, key_size)\n",
        "        self.linear_values = nn.Linear(in_channels, value_size)\n",
        "        self.sqrt_key_size = math.sqrt(key_size)\n",
        "\n",
        "    def forward(self, input):\n",
        "        # input is dim (N, T, in_channels) where N is the batch_size, and T is\n",
        "        # the sequence length\n",
        "        mask = np.array([[1 if i>j else 0 for i in range(input.shape[1])] for j in range(input.shape[1])])\n",
        "        mask = torch.ByteTensor(mask).cuda()\n",
        "\n",
        "        #import pdb; pdb.set_trace()\n",
        "        keys = self.linear_keys(input) # shape: (N, T, key_size)\n",
        "        query = self.linear_query(input) # shape: (N, T, key_size)\n",
        "        values = self.linear_values(input) # shape: (N, T, value_size)\n",
        "        temp = torch.bmm(query, torch.transpose(keys, 1, 2)) # shape: (N, T, T)\n",
        "        temp.data.masked_fill_(mask, -float('inf'))\n",
        "        temp = F.softmax(temp / self.sqrt_key_size, dim=1) # shape: (N, T, T), broadcasting over any slice [:, x, :], each row of the matrix\n",
        "        temp = torch.bmm(temp, values) # shape: (N, T, value_size)\n",
        "        return torch.cat((input, temp), dim=2) # shape: (N, T, in_channels + value_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1sQ7TtlFtJC"
      },
      "source": [
        "import numpy as np\n",
        "from scipy.interpolate import RegularGridInterpolator\n",
        "def transformation(inputImg, globalScale, rotationAngle, translationM, translationN, outputGridSize, method=\"linear\"):\n",
        "    \n",
        "    # Dimensions of the fixed image\n",
        "    mDim, nDim = outputGridSize\n",
        "    # Dimensions of the moving image\n",
        "    mDimOriginal, nDimOriginal = np.shape(inputImg)\n",
        "\n",
        "    # Define ranges and meshgrid with respect to the dimensions of the fixed image\n",
        "    mDimRange = np.linspace(-mDim//2, mDim//2, mDim)\n",
        "    nDimRange = np.linspace(-nDim//2, nDim//2, nDim)\n",
        "    mv, nv = np.array(np.meshgrid(mDimRange, nDimRange, indexing='ij'))\n",
        "\n",
        "    angleInRads = np.radians(rotationAngle)\n",
        "    rotationMatrix = np.array([[np.cos(angleInRads), -np.sin(angleInRads)], [np.sin(angleInRads), np.cos(angleInRads)]])\n",
        "    \n",
        "    # Scaled, rotated, and translated m and n coordinates \n",
        "    mTransformedCoords = (globalScale*rotationMatrix[0,0]*mv + globalScale*rotationMatrix[0,1]*nv) + translationM\n",
        "    nTransformedCoords = (globalScale*rotationMatrix[1,0]*mv + globalScale*rotationMatrix[1,1]*nv) + translationN\n",
        "    \n",
        "    # Define ranges with respect to the dimensions of the fixed image\n",
        "    mDimRangeOriginal = np.linspace(-mDimOriginal//2, mDimOriginal//2, mDimOriginal)\n",
        "    nDimRangeOriginal = np.linspace(-nDimOriginal//2, nDimOriginal//2, nDimOriginal)\n",
        "    \n",
        "    # Generate interpolator that will determine values of the moved image\n",
        "    # based on the intensity values of the moving image \n",
        "    interpolate = RegularGridInterpolator((mDimRangeOriginal, nDimRangeOriginal), inputImg, bounds_error=False, fill_value= 0, method=method);\n",
        "    # Stack the meshgrid to create an array of coordinate pairs to feed \n",
        "    # into the interpolator. The output is the moved image. \n",
        "    movedImage = interpolate(np.stack((mTransformedCoords, nTransformedCoords), axis=2))\n",
        "    return movedImage"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-L0hR-LA7m_"
      },
      "source": [
        "from __future__ import print_function\n",
        "import torch.utils.data as data\n",
        "import numpy as np\n",
        "import errno\n",
        "import os\n",
        "from PIL import Image, ImageOps, ImageFilter\n",
        "import torch\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt\n",
        "import imageio\n",
        "import cv2\n",
        "\n",
        "'''\n",
        "Inspired by https://github.com/pytorch/vision/pull/46\n",
        "'''\n",
        "\n",
        "IMG_CACHE = {}\n",
        "\n",
        "\n",
        "class OmniglotDataset(data.Dataset):\n",
        "    vinalys_baseurl = 'https://raw.githubusercontent.com/jakesnell/prototypical-networks/master/data/omniglot/splits/vinyals/'\n",
        "    vinyals_split_sizes = {\n",
        "        'test': vinalys_baseurl + 'test.txt',\n",
        "        'train': vinalys_baseurl + 'train.txt',\n",
        "        'trainval': vinalys_baseurl + 'trainval.txt',\n",
        "        'val': vinalys_baseurl + 'val.txt',\n",
        "    }\n",
        "\n",
        "    urls = [\n",
        "        'https://github.com/brendenlake/omniglot/raw/master/python/images_background.zip',\n",
        "        'https://github.com/brendenlake/omniglot/raw/master/python/images_evaluation.zip'\n",
        "    ]\n",
        "    splits_folder = os.path.join('splits', 'vinyals')\n",
        "    raw_folder = 'raw'\n",
        "    processed_folder = 'data'\n",
        "\n",
        "    def __init__(self, mode='train', root='../dataset/omniglot', transform=None, target_transform=None, download=True):\n",
        "        '''\n",
        "        The items are (filename,category). The index of all the categories can be found in self.idx_classes\n",
        "        Args:\n",
        "        - root: the directory where the dataset will be stored\n",
        "        - transform: how to transform the input\n",
        "        - target_transform: how to transform the target\n",
        "        - download: need to download the dataset\n",
        "        '''\n",
        "        super(OmniglotDataset, self).__init__()\n",
        "        self.mode = mode\n",
        "        self.root = root\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "        if download:\n",
        "            self.download()\n",
        "\n",
        "        if not self._check_exists():\n",
        "            raise RuntimeError(\n",
        "                'Dataset not found. You can use download=True to download it')\n",
        "        \n",
        "        if mode == 'test':\n",
        "          self.root = '../dataset/QuickDrawData'\n",
        "          self.classes = os.listdir(self.root)\n",
        "          print(self.classes)\n",
        "          self.all_items = find_items(self.root, self.classes,mode)\n",
        "\n",
        "          self.idx_classes = index_classes(self.all_items)\n",
        "#####################\n",
        "        else: \n",
        "          self.classes = get_current_classes(os.path.join(\n",
        "              self.root, self.splits_folder, mode + '.txt'))\n",
        "          self.all_items = find_items(os.path.join(\n",
        "              self.root, self.processed_folder), self.classes,mode)\n",
        "\n",
        "          self.idx_classes = index_classes(self.all_items)\n",
        "\n",
        "        paths, self.y = zip(*[self.get_path_label(pl)\n",
        "                              for pl in range(len(self))])\n",
        "        \n",
        "        modes = []\n",
        "        for pl in range(len(self)):\n",
        "          modes.append(self.mode)\n",
        "\n",
        "        self.x = map(load_img, paths, range(len(paths)), modes)\n",
        "        self.x = list(self.x)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.x[idx]\n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "        return x, self.y[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.all_items)\n",
        "\n",
        "    def get_path_label(self, index):\n",
        "        if self.mode == 'test':\n",
        "          filename = self.all_items[index][0]\n",
        "          img = str.join('/', [self.all_items[index][2], filename])\n",
        "          target = self.idx_classes[self.all_items[index]\n",
        "                                    [1] + self.all_items[index][-1]]\n",
        "          if self.target_transform is not None:\n",
        "              target = self.target_transform(target)\n",
        "        else:\n",
        "          filename = self.all_items[index][0]\n",
        "          rot = self.all_items[index][-1]\n",
        "          img = str.join('/', [self.all_items[index][2], filename]) + rot\n",
        "          target = self.idx_classes[self.all_items[index]\n",
        "                                    [1] + self.all_items[index][-1]]\n",
        "          if self.target_transform is not None:\n",
        "              target = self.target_transform(target)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def _check_exists(self):\n",
        "        return os.path.exists(os.path.join(self.root, self.processed_folder))\n",
        "\n",
        "    def download(self):\n",
        "        from six.moves import urllib\n",
        "        import zipfile\n",
        "\n",
        "        if self._check_exists():\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            os.makedirs(os.path.join(self.root, self.splits_folder))\n",
        "            os.makedirs(os.path.join(self.root, self.raw_folder))\n",
        "            os.makedirs(os.path.join(self.root, self.processed_folder))\n",
        "        except OSError as e:\n",
        "            if e.errno == errno.EEXIST:\n",
        "                pass\n",
        "            else:\n",
        "                raise\n",
        "\n",
        "        for k, url in self.vinyals_split_sizes.items():\n",
        "            print('== Downloading ' + url)\n",
        "            data = urllib.request.urlopen(url)\n",
        "            filename = url.rpartition('/')[-1]\n",
        "            file_path = os.path.join(self.root, self.splits_folder, filename)\n",
        "            with open(file_path, 'wb') as f:\n",
        "                f.write(data.read())\n",
        "\n",
        "        for url in self.urls:\n",
        "            print('== Downloading ' + url)\n",
        "            data = urllib.request.urlopen(url)\n",
        "            filename = url.rpartition('/')[2]\n",
        "            file_path = os.path.join(self.root, self.raw_folder, filename)\n",
        "            with open(file_path, 'wb') as f:\n",
        "                f.write(data.read())\n",
        "            orig_root = os.path.join(self.root, self.raw_folder)\n",
        "            print(\"== Unzip from \" + file_path + \" to \" + orig_root)\n",
        "            zip_ref = zipfile.ZipFile(file_path, 'r')\n",
        "            zip_ref.extractall(orig_root)\n",
        "            zip_ref.close()\n",
        "        file_processed = os.path.join(self.root, self.processed_folder)\n",
        "        for p in ['images_background', 'images_evaluation']:\n",
        "            for f in os.listdir(os.path.join(orig_root, p)):\n",
        "                shutil.move(os.path.join(orig_root, p, f), file_processed)\n",
        "            os.rmdir(os.path.join(orig_root, p))\n",
        "        print(\"Download finished.\")\n",
        "\n",
        "\n",
        "def find_items(root_dir, classes, mode):\n",
        "\n",
        "    if mode == \"test\":\n",
        "      retour = []\n",
        "      for (root, dirs, files) in os.walk(root_dir):\n",
        "          for f in files:\n",
        "              r = root.split('/')\n",
        "              lr = len(r)\n",
        "              label = r[lr - 1]\n",
        "              if label in classes and (f.endswith(\"png\")):\n",
        "                  retour.extend([(f, label, root)])\n",
        "      print(\"== Dataset: Found %d items \" % len(retour))      \n",
        "    else:\n",
        "      retour = []\n",
        "      rots = ['/rot000', '/rot090', '/rot180', '/rot270']\n",
        "      for (root, dirs, files) in os.walk(root_dir):\n",
        "          for f in files:\n",
        "              r = root.split('/')\n",
        "              lr = len(r)\n",
        "              label = r[lr - 2] + \"/\" + r[lr - 1]\n",
        "              for rot in rots:\n",
        "                  if label + rot in classes and (f.endswith(\"png\")):\n",
        "                      retour.extend([(f, label, root, rot)])\n",
        "      print(\"== Dataset: Found %d items \" % len(retour))\n",
        "    return retour\n",
        "\n",
        "\n",
        "def index_classes(items):\n",
        "    idx = {}\n",
        "    for i in items:\n",
        "        if (not i[1] + i[-1] in idx):\n",
        "            idx[i[1] + i[-1]] = len(idx)\n",
        "    print(\"== Dataset: Found %d classes\" % len(idx))\n",
        "    return idx\n",
        "\n",
        "\n",
        "def get_current_classes(fname):\n",
        "    with open(fname) as f:\n",
        "        classes = f.read().splitlines()\n",
        "    return classes\n",
        "\n",
        "def image_file_to_array(filename, dim_input):\n",
        "  \"\"\"\n",
        "  Takes an image path and returns numpy array\n",
        "  Args:\n",
        "    filename: Image filename\n",
        "    dim_input: Flattened shape of image\n",
        "  Returns:\n",
        "    1 channel image\n",
        "  \"\"\"\n",
        "  image = imageio.imread(filename)\n",
        "  image = image.reshape([dim_input])\n",
        "  image = image.astype(np.float32) / 255.0\n",
        "  image = 1.0 - image\n",
        "  return image\n",
        "\n",
        "def load_img(path, idx, mode):\n",
        "    if mode != 'test':\n",
        "      path, rot = path.split('/rot')\n",
        "    if path in IMG_CACHE:\n",
        "        x = IMG_CACHE[path]\n",
        "    else:\n",
        "        x = Image.open(path)\n",
        "        IMG_CACHE[path] = x\n",
        "    if mode != 'test':\n",
        "      x = x.rotate(float(rot))\n",
        "      x = x.resize((28, 28))\n",
        "\n",
        "    shape = 1, x.size[0], x.size[1]\n",
        "    x = np.array(x, np.float32, copy=False)\n",
        "    x = 1.0 - torch.from_numpy(x)\n",
        "    x = x.transpose(0, 1).contiguous().view(shape)\n",
        "\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oafjWbnDBUG6"
      },
      "source": [
        "from collections import OrderedDict\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def conv(in_planes, out_planes, kernel_size=3, stride=1, padding=1):\n",
        "    \"\"\"convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride,\n",
        "                     padding=padding, bias=False)\n",
        "\n",
        "def conv_block(in_channels, out_channels):\n",
        "    '''\n",
        "    returns a block conv-bn-relu-pool\n",
        "    '''\n",
        "    return nn.Sequential(OrderedDict([\n",
        "        ('conv', nn.Conv2d(in_channels, out_channels, 3, padding=1)),\n",
        "        ('bn', nn.BatchNorm2d(out_channels, momentum=1)),\n",
        "        #('bn', nn.BatchNorm2d(out_channels)),\n",
        "        ('relu', nn.ReLU()),\n",
        "        ('pool', nn.MaxPool2d(2))\n",
        "    ]))\n",
        "\n",
        "def batchnorm(input, weight=None, bias=None, running_mean=None, running_var=None, training=True,eps=1e-5, momentum=0.1):\n",
        "    # momentum = 1 restricts stats to the current mini-batch\n",
        "    # This hack only works when momentum is 1 and avoids needing to track\n",
        "    # running stats by substituting dummy variables\n",
        "    size = int(np.prod(np.array(input.data.size()[1])))\n",
        "    running_mean = torch.zeros(size).cuda()\n",
        "    running_var = torch.ones(size).cuda()\n",
        "    return F.batch_norm(input, running_mean, running_var, weight, bias, training, momentum, eps)\n",
        "\n",
        "class OmniglotNet(nn.Module):\n",
        "    '''\n",
        "    Model as described in the reference paper,\n",
        "    source: https://github.com/jakesnell/prototypical-networks/blob/f0c48808e496989d01db59f86d4449d7aee9ab0c/protonets/models/few_shot.py#L62-L84\n",
        "    '''\n",
        "    def __init__(self, x_dim=1, hid_dim=64, z_dim=64):\n",
        "        super(OmniglotNet, self).__init__()\n",
        "        self.encoder = nn.Sequential(OrderedDict([\n",
        "            ('block1', conv_block(x_dim, hid_dim)),\n",
        "            ('block2', conv_block(hid_dim, hid_dim)),\n",
        "            ('block3', conv_block(hid_dim, hid_dim)),\n",
        "            ('block4', conv_block(hid_dim, z_dim)),\n",
        "        ]))\n",
        "\n",
        "    def forward(self, x, weights=None):\n",
        "        if weights is None:\n",
        "            x = self.encoder(x)\n",
        "        else:\n",
        "            x = F.conv2d(x, weights['encoder.block1.conv.weight'], weights['encoder.block1.conv.bias'])\n",
        "            x = batchnorm(x, weight=weights['encoder.block1.bn.weight'], bias=weights['encoder.block1.bn.bias'])\n",
        "            x = F.relu(x)\n",
        "            x = F.max_pool2d(x, 2, 2)\n",
        "            x = F.conv2d(x, weights['encoder.block2.conv.weight'], weights['encoder.block2.conv.bias'])\n",
        "            x = batchnorm(x, weight=weights['encoder.block2.bn.weight'], bias=weights['encoder.block2.bn.bias'])\n",
        "            x = F.relu(x)\n",
        "            x = F.max_pool2d(x, 2, 2)\n",
        "            x = F.conv2d(x, weights['encoder.block3.conv.weight'], weights['encoder.block3.conv.bias'])\n",
        "            x = batchnorm(x, weight=weights['encoder.block3.bn.weight'], bias=weights['encoder.block3.bn.bias'])\n",
        "            x = F.relu(x)\n",
        "            x = F.max_pool2d(x, 2, 2)\n",
        "            x = F.conv2d(x, weights['encoder.block4.conv.weight'], weights['encoder.block4.conv.bias'])\n",
        "            x = batchnorm(x, weight=weights['encoder.block4.bn.weight'], bias=weights['encoder.block4.bn.bias'])\n",
        "            x = F.relu(x)\n",
        "            x = F.max_pool2d(x, 2, 2)\n",
        "        return x.view(x.size(0), -1)\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, filters, pool_padding=0):\n",
        "        super(ResBlock, self).__init__()\n",
        "        self.conv1 = conv(in_channels, filters)\n",
        "        self.bn1 = nn.BatchNorm2d(filters)\n",
        "        self.relu1 = nn.LeakyReLU()\n",
        "        self.conv2 = conv(filters, filters)\n",
        "        self.bn2 = nn.BatchNorm2d(filters)\n",
        "        self.relu2 = nn.LeakyReLU()\n",
        "        self.conv3 = conv(filters, filters)\n",
        "        self.bn3 = nn.BatchNorm2d(filters)\n",
        "        self.relu3 = nn.LeakyReLU()\n",
        "        self.conv4 = conv(in_channels, filters, kernel_size=1, padding=0)\n",
        "\n",
        "        self.maxpool = nn.MaxPool2d(2, padding=pool_padding)\n",
        "        self.dropout = nn.Dropout(p=0.9)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = self.conv4(x)\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu1(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu2(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "        out = self.relu3(out)\n",
        "\n",
        "        out += residual\n",
        "        out = self.maxpool(out)\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class MiniImagenetNet(nn.Module):\n",
        "    '''\n",
        "    Model as described in the reference paper,\n",
        "    source: https://github.com/jakesnell/prototypical-networks/blob/f0c48808e496989d01db59f86d4449d7aee9ab0c/protonets/models/few_shot.py#L62-L84\n",
        "    '''\n",
        "    def __init__(self, in_channels=3):\n",
        "        super(MiniImagenetNet, self).__init__()\n",
        "        self.block1 = ResBlock(in_channels, 64)\n",
        "        self.block2 = ResBlock(64, 96)\n",
        "        self.block3 = ResBlock(96, 128, pool_padding=1)\n",
        "        self.block4 = ResBlock(128, 256, pool_padding=1)\n",
        "        self.conv1 = conv(256, 2048, kernel_size=1, padding=0)\n",
        "        self.maxpool = nn.MaxPool2d(6)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=0.9)\n",
        "        self.conv2 = conv(2048, 384, kernel_size=1, padding=0)\n",
        "        \n",
        "    def forward(self, x, weights=None):\n",
        "        if weights is None:\n",
        "            x = self.block1(x)\n",
        "            x = self.block2(x)\n",
        "            x = self.block3(x)\n",
        "            x = self.block4(x)\n",
        "            x = self.conv1(x)\n",
        "            x = self.maxpool(x)\n",
        "            x = self.relu(x)\n",
        "            x = self.dropout(x)\n",
        "            x = self.conv2(x)\n",
        "        else:\n",
        "            raise ValueError('Not implemented yet')\n",
        "        return x.view(x.size(0), -1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlc7Oj_ABcd_"
      },
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SnailFewShot(nn.Module):\n",
        "    def __init__(self, N, K, task, use_cuda=True):\n",
        "        # N-way, K-shot\n",
        "        super(SnailFewShot, self).__init__()\n",
        "        if task == 'omniglot':\n",
        "            self.encoder = OmniglotNet()\n",
        "            num_channels = 64 + N\n",
        "        elif task == 'mini_imagenet':\n",
        "            self.encoder = MiniImagenetNet()\n",
        "            num_channels = 384 + N\n",
        "        else:\n",
        "            raise ValueError('Not recognized task value')\n",
        "        num_filters = int(math.ceil(math.log(N * K + 1, 2)))\n",
        "        self.attention1 = AttentionBlock(num_channels, 64, 32)\n",
        "        num_channels += 32\n",
        "        self.tc1 = TCBlock(num_channels, N * K + 1, 128)\n",
        "        num_channels += num_filters * 128\n",
        "        self.attention2 = AttentionBlock(num_channels, 256, 128)\n",
        "        num_channels += 128\n",
        "        self.tc2 = TCBlock(num_channels, N * K + 1, 128)\n",
        "        num_channels += num_filters * 128\n",
        "        self.attention3 = AttentionBlock(num_channels, 512, 256)\n",
        "        num_channels += 256\n",
        "        self.fc = nn.Linear(num_channels, N)\n",
        "        self.N = N\n",
        "        self.K = K\n",
        "        self.use_cuda = use_cuda\n",
        "\n",
        "    def forward(self, input, labels):\n",
        "        x = self.encoder(input)\n",
        "        batch_size = int(labels.size()[0] / (self.N * self.K + 1))\n",
        "        last_idxs = [(i + 1) * (self.N * self.K + 1) - 1 for i in range(batch_size)]\n",
        "        if self.use_cuda:\n",
        "            labels[last_idxs] = torch.Tensor(np.zeros((batch_size, labels.size()[1]))).cuda()\n",
        "        else:\n",
        "            labels[last_idxs] = torch.Tensor(np.zeros((batch_size, labels.size()[1])))\n",
        "        x = torch.cat((x, labels), 1)\n",
        "        x = x.view((batch_size, self.N * self.K + 1, -1))\n",
        "        x = self.attention1(x)\n",
        "        x = self.tc1(x)\n",
        "        x = self.attention2(x)\n",
        "        x = self.tc2(x)\n",
        "        x = self.attention3(x)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvsxQWI2C1A7"
      },
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "def init_dataset(opt):\n",
        "    '''\n",
        "    Initialize the datasets, samplers and dataloaders\n",
        "    '''\n",
        "    if opt.dataset == 'omniglot':\n",
        "        train_dataset = OmniglotDataset(mode='train')\n",
        "        val_dataset = OmniglotDataset(mode='val')\n",
        "        trainval_dataset = OmniglotDataset(mode='trainval')\n",
        "        test_dataset = OmniglotDataset(mode='test')\n",
        "    elif opt.dataset == 'mini_imagenet':\n",
        "        train_dataset = MiniImagenetDataset(mode='train')\n",
        "        val_dataset = MiniImagenetDataset(mode='val')\n",
        "        trainval_dataset = MiniImagenetDataset(mode='val')\n",
        "        test_dataset = MiniImagenetDataset(mode='test')\n",
        "\n",
        "    tr_sampler = BatchSampler(labels=train_dataset.y,\n",
        "                                          classes_per_it=opt.num_cls,\n",
        "                                          num_samples=opt.num_samples,\n",
        "                                          iterations=opt.iterations,\n",
        "                                          batch_size=opt.batch_size)\n",
        "\n",
        "    val_sampler = BatchSampler(labels=val_dataset.y,\n",
        "                                           classes_per_it=opt.num_cls,\n",
        "                                           num_samples=opt.num_samples,\n",
        "                                           iterations=opt.iterations,\n",
        "                                           batch_size=opt.batch_size)\n",
        "\n",
        "    trainval_sampler = BatchSampler(labels=trainval_dataset.y,\n",
        "                                                classes_per_it=opt.num_cls,\n",
        "                                                num_samples=opt.num_samples,\n",
        "                                                iterations=opt.iterations,\n",
        "                                                batch_size=opt.batch_size)\n",
        "\n",
        "    test_sampler = BatchSampler(labels=test_dataset.y,\n",
        "                                            classes_per_it=opt.num_cls,\n",
        "                                            num_samples=opt.num_samples,\n",
        "                                            iterations=opt.iterations,\n",
        "                                            batch_size=opt.batch_size)\n",
        "\n",
        "    tr_dataloader = torch.utils.data.DataLoader(train_dataset,\n",
        "                                                batch_sampler=tr_sampler)\n",
        "\n",
        "    val_dataloader = torch.utils.data.DataLoader(val_dataset,\n",
        "                                                 batch_sampler=val_sampler)\n",
        "\n",
        "    trainval_dataloader = torch.utils.data.DataLoader(trainval_dataset,\n",
        "                                                      batch_sampler=trainval_sampler)\n",
        "\n",
        "    test_dataloader = torch.utils.data.DataLoader(test_dataset,\n",
        "                                                  batch_sampler=test_sampler)\n",
        "     \n",
        "    return tr_dataloader, val_dataloader, trainval_dataloader, test_dataloader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fy_dhBuBogj"
      },
      "source": [
        "# coding=utf-8\n",
        "import argparse\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import copy\n",
        "import easydict\n",
        "\n",
        "\n",
        "def init_model(opt):\n",
        "    model = SnailFewShot(opt.num_cls, opt.num_samples, opt.dataset)\n",
        "    model = model.cuda() if opt.cuda else model\n",
        "    return model\n",
        "\n",
        "def save_list_to_file(path, thelist):\n",
        "    with open(path, 'w') as f:\n",
        "        for item in thelist:\n",
        "            f.write(\"%s\\n\" % item)\n",
        "\n",
        "def labels_to_one_hot(opt, labels):\n",
        "    if opt.cuda:\n",
        "        labels = labels.cpu()\n",
        "    labels = labels.numpy()\n",
        "    unique = np.unique(labels)\n",
        "    map = {label:idx for idx, label in enumerate(unique)}\n",
        "    idxs = [map[labels[i]] for i in range(labels.size)]\n",
        "    one_hot = np.zeros((labels.size, unique.size))\n",
        "    one_hot[np.arange(labels.size), idxs] = 1\n",
        "    return one_hot, idxs\n",
        "\n",
        "def batch_for_few_shot(opt, x, y):\n",
        "    seq_size = opt.num_cls * opt.num_samples + 1\n",
        "    one_hots = []\n",
        "    last_targets = []\n",
        "    for i in range(opt.batch_size):\n",
        "        one_hot, idxs = labels_to_one_hot(opt, y[i * seq_size: (i + 1) * seq_size])\n",
        "        one_hots.append(one_hot)\n",
        "        last_targets.append(idxs[-1])\n",
        "    last_targets = Variable(torch.Tensor(last_targets).long())\n",
        "    one_hots = [torch.Tensor(temp) for temp in one_hots]\n",
        "    y = torch.cat(one_hots, dim=0)\n",
        "    x, y = Variable(x), Variable(y)\n",
        "    if opt.cuda:\n",
        "        x, y = x.cuda(), y.cuda()\n",
        "        last_targets = last_targets.cuda()\n",
        "    return x, y, last_targets\n",
        "\n",
        "def get_acc(last_model, last_targets):\n",
        "    _, preds = last_model.max(1)\n",
        "    acc = torch.eq(preds, last_targets).float().mean()\n",
        "    return acc.item()\n",
        "\n",
        "def train(opt, tr_dataloader, model, optim, val_dataloader=None):\n",
        "    if val_dataloader is None:\n",
        "        best_state = None\n",
        "    train_loss = []\n",
        "    train_acc = []\n",
        "    val_loss = []\n",
        "    val_acc = []\n",
        "    best_acc = 0\n",
        "\n",
        "    best_model_path = os.path.join(opt.exp, 'best_model.pth')\n",
        "    last_model_path = os.path.join(opt.exp, 'last_model.pth')\n",
        "\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(opt.epochs):\n",
        "        print('=== Epoch: {} ==='.format(epoch))\n",
        "        tr_iter = iter(tr_dataloader)\n",
        "        model.train()\n",
        "        model = model.cuda()\n",
        "        for batch in tqdm(tr_iter):\n",
        "            optim.zero_grad()\n",
        "            x, y = batch\n",
        "            x, y, last_targets = batch_for_few_shot(opt, x, y)\n",
        "            model_output = model(x, y)\n",
        "            last_model = model_output[:, -1, :]\n",
        "            loss = loss_fn(last_model, last_targets)\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            train_loss.append(loss.item())\n",
        "            train_acc.append(get_acc(last_model, last_targets))\n",
        "        avg_loss = np.mean(train_loss[-opt.iterations:])\n",
        "        avg_acc = np.mean(train_acc[-opt.iterations:])\n",
        "        print('Avg Train Loss: {}, Avg Train Acc: {}'.format(avg_loss, avg_acc))\n",
        "        if val_dataloader is None:\n",
        "            continue\n",
        "        val_iter = iter(val_dataloader)\n",
        "        model.eval()\n",
        "        for batch in val_iter:\n",
        "            x, y = batch\n",
        "            x, y, last_targets = batch_for_few_shot(opt, x, y)\n",
        "            model_output = model(x, y)\n",
        "            last_model = model_output[:, -1, :]\n",
        "            loss = loss_fn(last_model, last_targets)\n",
        "            val_loss.append(loss.item())\n",
        "            val_acc.append(get_acc(last_model, last_targets))\n",
        "        avg_loss = np.mean(val_loss[-opt.iterations:])\n",
        "        avg_acc = np.mean(val_acc[-opt.iterations:])\n",
        "        postfix = ' (Best)' if avg_acc >= best_acc else ' (Best: {})'.format(\n",
        "            best_acc)\n",
        "        print('Avg Val Loss: {}, Avg Val Acc: {}{}'.format(\n",
        "            avg_loss, avg_acc, postfix))\n",
        "        if avg_acc >= best_acc:\n",
        "            torch.save(model.state_dict(), best_model_path)\n",
        "            best_acc = avg_acc\n",
        "            best_state = model.state_dict()\n",
        "        for name in ['train_loss', 'train_acc', 'val_loss', 'val_acc']:\n",
        "            save_list_to_file(os.path.join(opt.exp, name + '.txt'), locals()[name])\n",
        "\n",
        "    torch.save(model.state_dict(), last_model_path)\n",
        "\n",
        "    return best_state, best_acc, train_loss, train_acc, val_loss, val_acc\n",
        "\n",
        "\n",
        "def test(opt, test_dataloader, model):\n",
        "    avg_acc = list()\n",
        "    for epoch in range(10):\n",
        "        test_iter = iter(test_dataloader)\n",
        "        for batch in test_iter:\n",
        "            x, y = batch\n",
        "            x, y, last_targets = batch_for_few_shot(opt, x, y)\n",
        "            model_output = model(x, y)\n",
        "            last_model = model_output[:, -1, :]\n",
        "            avg_acc.append(get_acc(last_model, last_targets))\n",
        "    avg_acc = np.mean(avg_acc)\n",
        "    print('Test Acc: {}'.format(avg_acc))\n",
        "\n",
        "    return avg_acc\n",
        "\n",
        "def main():\n",
        "    '''\n",
        "    Initialize everything and train\n",
        "    '''\n",
        "\n",
        "    options = easydict.EasyDict({\n",
        "    \"exp\": 'default',\n",
        "    \"epochs\": 7,\n",
        "    \"iterations\": 10000,\n",
        "    \"dataset\": 'omniglot',\n",
        "    \"num_cls\": 5,\n",
        "    \"num_samples\": 1,\n",
        "    \"lr\": 0.0001,\n",
        "    \"batch_size\": 32,\n",
        "    \"cuda\": True\n",
        "    })\n",
        "\n",
        "    if not os.path.exists(options.exp):\n",
        "        os.makedirs(options.exp)\n",
        "\n",
        "    if torch.cuda.is_available() and not options.cuda:\n",
        "        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
        "\n",
        "    tr_dataloader, val_dataloader, trainval_dataloader, test_dataloader = init_dataset(options)\n",
        "    model = init_model(options)\n",
        "    optim = torch.optim.Adam(params=model.parameters(), lr=options.lr)\n",
        "    res = train(opt=options,\n",
        "                tr_dataloader=tr_dataloader,\n",
        "                val_dataloader=val_dataloader,\n",
        "                model=model,\n",
        "                optim=optim)\n",
        "    best_state, best_acc, train_loss, train_acc, val_loss, val_acc = res\n",
        "\n",
        "    model.load_state_dict(best_state)\n",
        "    print('Testing with best model..')\n",
        "    test(opt=options,\n",
        "         test_dataloader=test_dataloader,\n",
        "         model=model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26q0BSNRCgL4"
      },
      "source": [
        "main()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}